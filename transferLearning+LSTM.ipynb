{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRuYQVxJ3BomosEeHFUELG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaibhav-prasad707/DAA_707/blob/main/transferLearning%2BLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries\n"
      ],
      "metadata": {
        "id": "QC5d8OHVIbGd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jjhHad7LHznW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import (\n",
        "    ResNet50, resnet50,\n",
        "    VGG16, vgg16,\n",
        "    MobileNetV2, mobilenet_v2,\n",
        "    InceptionV3, inception_v3,\n",
        "    DenseNet121, densenet\n",
        ")\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Configurations"
      ],
      "metadata": {
        "id": "FGjiuAvgIfRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_CONFIG = {\n",
        "    'ResNet50': (ResNet50, resnet50.preprocess_input, (224, 224)),\n",
        "    'VGG16': (VGG16, vgg16.preprocess_input, (224, 224)),\n",
        "    'MobileNetV2': (MobileNetV2, mobilenet_v2.preprocess_input, (224, 224)),\n",
        "    'DenseNet121': (DenseNet121, densenet.preprocess_input, (224, 224)),\n",
        "    'InceptionV3': (InceptionV3, inception_v3.preprocess_input, (299, 299)),\n",
        "}"
      ],
      "metadata": {
        "id": "EVRgvZ_tH68F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mock data (For now)"
      ],
      "metadata": {
        "id": "Md9RlstzIk7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mock_data(num_samples, img_size, num_classes):\n",
        "    \"\"\"\n",
        "    Generates mock image data and labels.\n",
        "    \"\"\"\n",
        "    print(f\"Generating {num_samples} mock samples of size {img_size}...\")\n",
        "    # Generate random pixel data\n",
        "    x = np.random.rand(num_samples, img_size[0], img_size[1], 3).astype(np.float32)\n",
        "    # Generate random integer labels\n",
        "    y_int = np.random.randint(0, num_classes, size=num_samples)\n",
        "    # One-hot encode labels\n",
        "    y_one_hot = to_categorical(y_int, num_classes)\n",
        "    return x, y_one_hot"
      ],
      "metadata": {
        "id": "cldAPJuEH9kT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM Model"
      ],
      "metadata": {
        "id": "fjOWIqZpIsXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_hybrid_cnn_lstm_model(model_name, num_classes):\n",
        "    \"\"\"\n",
        "    Builds a modular hybrid CNN-LSTM model.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): The name of the model from MODEL_CONFIG.\n",
        "        num_classes (int): The number of output classes.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.models.Model: The compiled Keras model.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Get model configuration\n",
        "    if model_name not in MODEL_CONFIG:\n",
        "        raise ValueError(f\"Model {model_name} not found in MODEL_CONFIG.\")\n",
        "\n",
        "    ModelClass, preprocess_fn, img_size = MODEL_CONFIG[model_name]\n",
        "    input_shape = (img_size[0], img_size[1], 3)\n",
        "\n",
        "    # 2. Load pre-trained base model\n",
        "    base_model = ModelClass(\n",
        "        weights='imagenet',\n",
        "        include_top=False,     # Exclude the final classification layer\n",
        "        input_shape=input_shape\n",
        "    )\n",
        "    # Freeze the weights of the pre-trained layers\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # 3. Build the full hybrid model\n",
        "    inputs = layers.Input(shape=input_shape, name='input_image')\n",
        "\n",
        "    # 4. Apply model-specific preprocessing\n",
        "    # This Lambda layer bakes the preprocessing into the model\n",
        "    x = layers.Lambda(preprocess_fn, name='preprocessing')(inputs)\n",
        "\n",
        "    # 5. Pass through the frozen base model\n",
        "    x = base_model(x, training=False)\n",
        "\n",
        "    # 6. Reshape CNN output for LSTM\n",
        "    # The CNN base outputs a 4D feature map: (batch, height, width, channels)\n",
        "    # We need to reshape it into a 3D sequence for the LSTM: (batch, timesteps, features)\n",
        "    # We'll treat the spatial dimensions (height * width) as the \"timesteps\"\n",
        "\n",
        "    # Get the output shape of the base model\n",
        "    # e.g., (None, 7, 7, 2048) for ResNet50\n",
        "    cnn_output_shape = base_model.output_shape\n",
        "\n",
        "    # Calculate timesteps and features\n",
        "    # timesteps = height * width (e.g., 7 * 7 = 49)\n",
        "    # features = channels (e.g., 2048)\n",
        "    timesteps = cnn_output_shape[1] * cnn_output_shape[2]\n",
        "    features = cnn_output_shape[3]\n",
        "\n",
        "    # Reshape: (None, 7, 7, 2048) -> (None, 49, 2048)\n",
        "    x = layers.Reshape((timesteps, features), name='reshape_for_lstm')(x)\n",
        "\n",
        "    # 7. Add LSTM layer\n",
        "    # The LSTM will find temporal/spatial patterns in the features\n",
        "    x = layers.LSTM(64, name='lstm_layer')(x)\n",
        "\n",
        "    # 8. Add final classification layer\n",
        "    outputs = layers.Dense(num_classes, activation='softmax', name='classifier')(x)\n",
        "\n",
        "    # 9. Create and compile the final model\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            tf.keras.metrics.AUC(name='auc'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall')\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "7fJ-8qj_H_vX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment"
      ],
      "metadata": {
        "id": "pNnV-Fc_IxFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(model_name, num_classes, epochs, batch_size):\n",
        "    \"\"\"\n",
        "    Runs a full experiment for a single model.\n",
        "    \"\"\"\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Running experiment for: {model_name}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # 1. Get model-specific input size\n",
        "    img_size = MODEL_CONFIG[model_name][2]\n",
        "\n",
        "    # 2. Load data (using mock data here)\n",
        "    # Replace this with your actual data loading logic\n",
        "    # Make sure your real images are resized to `img_size` *before* a batch\n",
        "    x_train, y_train = get_mock_data(100, img_size, num_classes)\n",
        "    x_val, y_val = get_mock_data(20, img_size, num_classes)\n",
        "    x_test, y_test = get_mock_data(20, img_size, num_classes)\n",
        "\n",
        "    # 3. Create the model\n",
        "    model = create_hybrid_cnn_lstm_model(model_name, num_classes)\n",
        "\n",
        "    if model_name == 'ResNet50':\n",
        "        print(\"\\nModel Summary (ResNet50 example):\")\n",
        "        model.summary()\n",
        "\n",
        "    # 4. Train the model\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        validation_data=(x_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # 5. Evaluate the model\n",
        "    print(f\"\\nEvaluating {model_name}...\")\n",
        "    results = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
        "\n",
        "    # Format results into a dictionary\n",
        "    metrics = model.metrics_names\n",
        "    results_dict = dict(zip(metrics, results))\n",
        "\n",
        "    print(f\"Evaluation Results for {model_name}:\")\n",
        "    for key, value in results_dict.items():\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    return results_dict"
      ],
      "metadata": {
        "id": "bJk2hQiaIGf7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Comparative Analysis\n"
      ],
      "metadata": {
        "id": "Z06PzSssIOTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # --- Configuration ---\n",
        "    # Define which models you want to compare\n",
        "    MODELS_TO_COMPARE = ['MobileNetV2', 'ResNet50', 'DenseNet121']\n",
        "    NUM_CLASSES = 3  # e.g., (Nevus, Melanoma, Keratosis)\n",
        "    EPOCHS = 2       # Keep low for a quick test. Increase for real training.\n",
        "    BATCH_SIZE = 16\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    for model_name in MODELS_TO_COMPARE:\n",
        "        results = run_experiment(model_name, NUM_CLASSES, EPOCHS, BATCH_SIZE)\n",
        "        all_results[model_name] = results\n",
        "\n",
        "    # Print final comparative summary\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"           FINAL COMPARATIVE ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Convert results to a pandas DataFrame for nice printing\n",
        "    results_df = pd.DataFrame.from_dict(all_results, orient='index')\n",
        "    results_df = results_df.round(4)\n",
        "\n",
        "    print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Pq8iMMcvIMi6",
        "outputId": "792087a9-e0fa-4767-c86b-4fb280100939"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Running experiment for: MobileNetV2\n",
            "--------------------------------------------------------------------------------\n",
            "Generating 100 mock samples of size (224, 224)...\n",
            "Generating 20 mock samples of size (224, 224)...\n",
            "Generating 20 mock samples of size (224, 224)...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "\n",
            "Training MobileNetV2...\n",
            "Epoch 1/2\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - accuracy: 0.3485 - auc: 0.5764 - loss: 1.1458 - precision: 0.3626 - recall: 0.3014 - val_accuracy: 0.2000 - val_auc: 0.4469 - val_loss: 1.1392 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/2\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 687ms/step - accuracy: 0.4147 - auc: 0.5119 - loss: 1.1338 - precision: 0.2272 - recall: 0.0970 - val_accuracy: 0.2000 - val_auc: 0.3888 - val_loss: 1.2712 - val_precision: 0.2000 - val_recall: 0.2000\n",
            "\n",
            "Evaluating MobileNetV2...\n",
            "Evaluation Results for MobileNetV2:\n",
            "  loss: 1.2512\n",
            "  compile_metrics: 0.2000\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Running experiment for: ResNet50\n",
            "--------------------------------------------------------------------------------\n",
            "Generating 100 mock samples of size (224, 224)...\n",
            "Generating 20 mock samples of size (224, 224)...\n",
            "Generating 20 mock samples of size (224, 224)...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "\n",
            "Model Summary (ResNet50 example):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_image (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ preprocessing (\u001b[38;5;33mLambda\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape_for_lstm (\u001b[38;5;33mReshape\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m2048\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_layer (\u001b[38;5;33mLSTM\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m540,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ classifier (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ preprocessing (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape_for_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">540,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ classifier (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,128,835\u001b[0m (92.04 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,128,835</span> (92.04 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m541,123\u001b[0m (2.06 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">541,123</span> (2.06 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training ResNet50...\n",
            "Epoch 1/2\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 4s/step - accuracy: 0.3342 - auc: 0.5242 - loss: 1.1709 - precision: 0.3865 - recall: 0.1940 - val_accuracy: 0.1500 - val_auc: 0.3169 - val_loss: 1.3416 - val_precision: 0.1500 - val_recall: 0.1500\n",
            "Epoch 2/2\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 0.4522 - auc: 0.5981 - loss: 1.0660 - precision: 0.4805 - recall: 0.2777 - val_accuracy: 0.5000 - val_auc: 0.5706 - val_loss: 1.0910 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "\n",
            "Evaluating ResNet50...\n",
            "Evaluation Results for ResNet50:\n",
            "  loss: 1.1093\n",
            "  compile_metrics: 0.2500\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Running experiment for: DenseNet121\n",
            "--------------------------------------------------------------------------------\n",
            "Generating 100 mock samples of size (224, 224)...\n",
            "Generating 20 mock samples of size (224, 224)...\n",
            "Generating 20 mock samples of size (224, 224)...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m29084464/29084464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "\n",
            "Training DenseNet121...\n",
            "Epoch 1/2\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 4s/step - accuracy: 0.3911 - auc: 0.5670 - loss: 1.1433 - precision: 0.3015 - recall: 0.0436 - val_accuracy: 0.3500 - val_auc: 0.5362 - val_loss: 1.0963 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/2\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.4273 - auc: 0.5665 - loss: 1.1301 - precision: 0.1469 - recall: 0.0278 - val_accuracy: 0.3500 - val_auc: 0.5625 - val_loss: 1.1275 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "\n",
            "Evaluating DenseNet121...\n",
            "Evaluation Results for DenseNet121:\n",
            "  loss: 1.2353\n",
            "  compile_metrics: 0.2500\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "           FINAL COMPARATIVE ANALYSIS\n",
            "================================================================================\n",
            "               loss  compile_metrics\n",
            "MobileNetV2  1.2512             0.20\n",
            "ResNet50     1.1093             0.25\n",
            "DenseNet121  1.2353             0.25\n"
          ]
        }
      ]
    }
  ]
}